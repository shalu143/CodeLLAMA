{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifKzRZJbIzFC"
      },
      "source": [
        "Importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QArLZ-VxI1ea",
        "outputId": "f67c7780-d90a-41ad-f4a8-35a6ea1178f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5NmO5p6I3i-"
      },
      "source": [
        "Login into HuggingFace to download the Repo for Codellama\n",
        "\n",
        "Hugging face token: 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af1A7BwqI5p5",
        "outputId": "7e16f3d7-3462-45ff-a826-a436ad7c3245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz4fDsbvI7hK",
        "outputId": "d4fa283d-78b6-4b2b-854c-f8ff3b8e0e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CodeLlama-7b-Instruct-hf'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 64 (delta 1), reused 0 (delta 0), pack-reused 61\u001b[K\n",
            "Unpacking objects: 100% (64/64), 496.68 KiB | 1.67 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYPuKaDOI9lu"
      },
      "source": [
        "Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f0559695339845eaba4053ad7e30bcd5",
            "240ecf9a717b4241921aed601bb2afda",
            "cdc05ffa260c4d72bdf40be4f3b5e184",
            "ef821d69aed341d286d9eb4c24562163",
            "6afb62a83b8c416188fd4a35c952266e",
            "d4ba95c06549430e8b11fcd01bc5b82a",
            "9b9d56eb63e042fb9c80de1230b0289f",
            "8a0261337e864660a73268bb943de221",
            "b66f57389ab7480eb67445178ed57b93",
            "faf4c58570f74f28baf634c3019f42af",
            "248ce63d6cdc423f9f7ccfb5b288e1d6"
          ]
        },
        "id": "zwA1qz_vI_B2",
        "outputId": "b25be9ea-42f7-4a5a-d69d-6905b7a51b83"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0559695339845eaba4053ad7e30bcd5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"/content/CodeLlama-7b-Instruct-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeEBv5W0JD3Q"
      },
      "source": [
        "###Examples for Codellama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D46-IFSTJJmM",
        "outputId": "44440a53-c595-49de-b2b8-0acfb39d9fb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in pyspark\\n<</SYS>>\\n\\nPlot product sales data in bar chart[/INST]  To plot product sales data in a bar chart using PySpark, you can use the `bar` function from the `matplotlib` library. Here's an example of how you can do this:\n",
            "```\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Load the data into a DataFrame\n",
            "df = spark.read.csv(\"product_sales.csv\", header=True, inferSchema=True)\n",
            "\n",
            "# Group the data by product and calculate the total sales for each product\n",
            "df_grouped = df.groupBy(\"product\").agg(F.sum(\"sales\").alias(\"total_sales\"))\n",
            "\n",
            "# Plot the data as a bar chart\n",
            "plt.bar(df_grouped[\"product\"], df_grouped[\"total_sales\"])\n",
            "plt.xlabel(\"Product\")\n",
            "plt.ylabel(\"Total Sales\")\n",
            "plt.title(\"Product Sales\")\n",
            "plt.show()\n",
            "```\n",
            "This will create a bar chart with the product names on the x-axis and the total sales for each product on the y-axis. You can customize the appearance of the chart by using various options available in the `matplotlib` library, such as changing the colors, adding labels, and more.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in pyspark\"\n",
        "user = \"Plot product sales data in bar chart\"\n",
        "\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqsNmjKJKdK",
        "outputId": "227a9e23-4e0f-498c-a8e4-0bbe159eb45a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in pyspark\\n<</SYS>>\\n\\nVisualise product sales data in bar chart[/INST]  To visualize product sales data in a bar chart using PySpark, you can use the `matplotlib` library. Here's an example of how you can do this:\n",
            "```\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Load the data into a DataFrame\n",
            "df = spark.read.csv(\"product_sales.csv\", header=True, inferSchema=True)\n",
            "\n",
            "# Group the data by product and calculate the total sales for each product\n",
            "df_grouped = df.groupBy(\"product\").agg(F.sum(\"sales\").alias(\"total_sales\"))\n",
            "\n",
            "# Create a bar chart of the total sales for each product\n",
            "plt.bar(df_grouped[\"product\"], df_grouped[\"total_sales\"])\n",
            "plt.xlabel(\"Product\")\n",
            "plt.ylabel(\"Total Sales\")\n",
            "plt.title(\"Product Sales\")\n",
            "plt.show()\n",
            "```\n",
            "This will create a bar chart with the product names on the x-axis and the total sales for each product on the y-axis. You can customize the chart as needed by adding additional parameters to the `plt.bar()` function. For example, you can add a legend by using the `plt.legend()` function, or you can change the colors of the bars by using the `plt.bar()` function with the `color` parameter.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in pyspark\"\n",
        "user = \"Visualise product sales data in bar chart\"\n",
        "\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTaaFm7FJT05",
        "outputId": "25b58e7d-9bb0-42ec-eb14-d86951939176"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in pyspark\\n<</SYS>>\\n\\nHelp me write pyspark code to connect with Azure adls account abc and container pqr using a sas token to read file sales parquet from container folder sales_data and get product wise sales for each month[/INST]  Here is an example of how you can connect to an Azure ADLS account using a SAS token in PySpark:\n",
            "```\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "# Create a SparkSession\n",
            "spark = SparkSession.builder.appName(\"Azure ADLS\").getOrCreate()\n",
            "\n",
            "# Set the Azure ADLS account and container\n",
            "account = \"abc\"\n",
            "container = \"pqr\"\n",
            "\n",
            "# Set the SAS token\n",
            "sas_token = \"your_sas_token\"\n",
            "\n",
            "# Create a DataFrame from the Azure ADLS account and container\n",
            "df = spark.read.format(\"parquet\") \\\n",
            "   .option(\"path\", f\"abfss://{container}@{account}.dfs.core.windows.net/sales_data\") \\\n",
            "   .option(\"sasToken\", sas_token) \\\n",
            "   .load()\n",
            "\n",
            "# Group the DataFrame by product and month\n",
            "df_grouped = df.groupBy(\"product\", \"month\")\n",
            "\n",
            "# Calculate the total sales for each product and month\n",
            "df_grouped.agg(F.sum(\"sales\").alias(\"total_sales\"))\n",
            "```\n",
            "This code will read the parquet files from the `sales_data` folder in the `pqr` container in the `abc` Azure ADLS account using the SAS token. It will then group the DataFrame by product and month and calculate the total sales for each product and month.\n",
            "\n",
            "You can also use the `spark.read.format(\"parquet\").option(\"path\", \"abfss://{container}@{account}.dfs.core.windows.net/sales_data\").option(\"sasToken\", sas_token).load()` method to read the parquet files directly from the Azure ADLS account using the SAS token.\n",
            "\n",
            "You can also use the `spark.read.format(\"parquet\").option(\"path\", \"abfss://{container}@{account}.dfs.core.windows.net/sales_data\").option(\"sasToken\", sas_token).load()` method to read the parquet files directly from the Azure ADLS account using the SAS token.\n",
            "\n",
            "You can also use the `spark.read.format(\"parquet\").option(\"path\", \"abfss://{container}@{account}.dfs.core.windows.net/sales_data\").option(\"sasToken\", sas_token).load()` method to read the parquet files directly from the Azure ADLS account using the SAS token.\n",
            "\n",
            "You can also use the `spark.read.format(\"parquet\").option(\"path\", \"abfss://{container}@{account}.dfs.core.windows.net/sales_data\").option(\"sasToken\", sas_token).load()` method to read the parquet files directly from the Azure ADLS account using the SAS token.\n",
            "\n",
            "You can also use the `spark.read.format(\"parquet\").option(\"path\", \"abfss://{container}@{account}.dfs.core.windows.net/sales_data\").option(\"sasToken\", sas_token).load()` method to read the parquet files directly from the Azure ADLS account using the SAS token.\n",
            "\n",
            "You can also use the `spark.read.format(\"parquet\").option(\"path\", \"abfss://{container}@{account}.dfs.core.windows.net/sales_data\").option(\"sasToken\", sas_token).load()` method to read the parquet files directly from the Azure ADLS account using the SAS token.\n",
            "\n",
            "You can also use the `spark.read.format(\"parquet\").option(\"path\", \"abfss://{container}@{account}.dfs.core.windows.net/sales_data\").option(\"sasToken\", sas_token).load()` method to read the parquet files directly from the Azure ADLS account using the SAS token.\n",
            "\n",
            "You can also use the `spark.read.format(\"parquet\").option(\"path\", \"abfss://{container}@{account}.dfs.core.windows.net/sales_data\").option(\"sasToken\", sas_token).load\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in pyspark\"\n",
        "user = \"Help me write pyspark code to connect with Azure adls account abc and container pqr using a sas token to read file sales parquet from container folder sales_data and get product wise sales for each month\"\n",
        "\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac8HUm1aJa73"
      },
      "source": [
        "###Natural Language to SQL Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0TpjOQOKqGJ"
      },
      "source": [
        "1st Schema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9QaqcSjJaBR"
      },
      "outputs": [],
      "source": [
        "table = \"CREATE TABLE sales ( sale_id number PRIMARY KEY, product_id number, customer_id number, salesperson_id number, sale_date DATE, quantity number, FOREIGN KEY (product_id) REFERENCES products(product_id), FOREIGN KEY (customer_id) REFERENCES customers(customer_id), FOREIGN KEY (salesperson_id) REFERENCES salespeople(salesperson_id)); CREATE TABLE product_suppliers ( supplier_id number PRIMARY KEY, product_id number, supply_price number, FOREIGN KEY (product_id) REFERENCES products(product_id)); CREATE TABLE customers ( customer_id number PRIMARY KEY, name text, address text ); CREATE TABLE salespeople ( salesperson_id number PRIMARY KEY, name text, region text ); CREATE TABLE product_suppliers ( supplier_id number PRIMARY KEY, product_id number, supply_price number );\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl0pSRKIJmPD",
        "outputId": "122098c7-2efe-43f2-b4de-8191afac6866"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT COUNT(sale_id)\n",
            "FROM sales\n",
            "WHERE sale_date = '2022-01-01';\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##Low level query\n",
        "question = 'Find how many items were sold on one specific date.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb-EaX_QJrLu",
        "outputId": "20879fb0-0523-4241-d56b-ac2fe9a999ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT COUNT(DISTINCT customer_id) FROM sales;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = 'Find how many unique customers data is present'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT1IAkZRJulM",
        "outputId": "b56a02e2-e2d8-44d3-bc48-610b3e59dc74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT product_id, COUNT(*) AS total_sales\n",
            "FROM sales\n",
            "GROUP BY product_id\n",
            "ORDER BY total_sales DESC\n",
            "LIMIT 1;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Medium Level Query\n",
        "question = 'Find the product which was sold the most'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "693zhQB4JzHS",
        "outputId": "38ca3214-8ccb-4132-9987-7f01848ed2de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT customers.name\n",
            "FROM customers\n",
            "JOIN sales ON customers.customer_id = sales.customer_id\n",
            "JOIN products ON sales.product_id = products.product_id\n",
            "WHERE products.product_id IS NOT NULL;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#complex level query\n",
        "question = 'Find the names of the customer who purchased some product'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qipDrtt_JzhF",
        "outputId": "166fdfbe-a251-4e79-ce44-42b96be916d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT customers.address\n",
            "FROM customers\n",
            "JOIN sales ON customers.customer_id = sales.customer_id\n",
            "JOIN products ON sales.product_id = products.product_id\n",
            "GROUP BY customers.customer_id\n",
            "ORDER BY COUNT(products.product_id) DESC\n",
            "LIMIT 1;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = 'Find the address of the customer who bought the maximum products'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WM15KLzJ8AU"
      },
      "source": [
        "2nd Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okv1VkG1J6kI"
      },
      "outputs": [],
      "source": [
        "\n",
        "table1= \"\"\"\n",
        "CREATE TABLE customer\n",
        "   (\n",
        "       custid VARCHAR(6),\n",
        "       fname VARCHAR(30),\n",
        "       mname VARCHAR(30),\n",
        "       ltname VARCHAR(30),\n",
        "       city VARCHAR(15),\n",
        "       mobileno VARCHAR(10),\n",
        "       occupation VARCHAR(10),\n",
        "       dob DATE,\n",
        "      CONSTRAINT customer_custid_pk PRIMARY KEY(custid)\n",
        "   );\n",
        "   CREATE TABLE branch\n",
        "   (\n",
        "    bid VARCHAR(6),\n",
        "    bname VARCHAR(30),\n",
        "    bcity VARCHAR(30),\n",
        "    CONSTRAINT branch_bid_pk PRIMARY KEY(bid)\n",
        "   );\n",
        "   CREATE TABLE account\n",
        "   (\n",
        "      acnumber VARCHAR(6),\n",
        "      custid  VARCHAR(6),\n",
        "      bid VARCHAR(6),\n",
        "      opening_balance INT(7),\n",
        "      aod DATE,\n",
        "      atype VARCHAR(10),\n",
        "      astatus VARCHAR(10),\n",
        "      CONSTRAINT account_acnumber_pk PRIMARY KEY(acnumber),\n",
        "      CONSTRAINT account_custid_fk FOREIGN KEY(custid) REFERENCES customer(custid),\n",
        "      CONSTRAINT account_bid_fk FOREIGN KEY(bid) REFERENCES branch(bid)\n",
        "    );\n",
        "    CREATE TABLE trandetails\n",
        "    (\n",
        "     tnumber VARCHAR(6),\n",
        "     acnumber VARCHAR(6),\n",
        "     dot DATE,\n",
        "     medium_of_transaction VARCHAR(20),\n",
        "     transaction_type VARCHAR(20),\n",
        "     transaction_amount INT(7),\n",
        "     CONSTRAINT trandetails_tnumber_pk PRIMARY KEY(tnumber),\n",
        "     CONSTRAINT trandetails_acnumber_fk FOREIGN KEY(acnumber) REFERENCES account(acnumber)\n",
        "    );\n",
        "    CREATE TABLE loan\n",
        "   (\n",
        "\n",
        "    custid VARCHAR(6),\n",
        "    bid VARCHAR(6),\n",
        "    loan_amount INT(7),\n",
        "    CONSTRAINT loan_customer_custid_bid_pk PRIMARY KEY(custid,bid),\n",
        "    CONSTRAINT loan_custid_fk FOREIGN KEY(custid) REFERENCES  customer(custid),\n",
        "    CONSTRAINT loan_bid_fk FOREIGN KEY(bid) REFERENCES  branch(bid)\n",
        "   );\n",
        "   \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjoMFUk7KAFA",
        "outputId": "ca3d86a1-848d-40cf-f3f5-1ec17263b5d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT custid, fname, dob\n",
            "FROM customer\n",
            "ORDER BY YEAR(dob), fname;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Low level query\n",
        "question = 'Write a query to display the customer number, firstname, customer’s date of birth. Display in sorted order of date of birth year and within that sort by firstname.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table1} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-21-1D_CKFdj",
        "outputId": "9ad1cafa-d979-40b2-deca-14d8528e9632"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT custid, fname, IFNULL(mname, ltname) AS Cust_Name\n",
            "FROM customer\n",
            "ORDER BY custid;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = 'Write a query to display the customer’s number, first name, and middle name. The customer’s who don’t have a middle name, for them display the last name. Give the alias name as Cust_Name.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table1} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYNUYWR5KI78",
        "outputId": "af270272-ea98-4fe8-c67b-6c26bc9338a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT b.bcity, COUNT(b.bcity) AS Count_Branch\n",
            "FROM branch b\n",
            "GROUP BY b.bcity;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = 'Write a query to display city name and count of branches in that city. Give the count of branches an alias name of Count_Branch.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table1} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEZ7jQDIKMv-",
        "outputId": "4ace598c-85cf-45cd-daad-5f36cf53c836"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT account.acnumber, customer.custid, customer.fname, customer.ltname, account.aod\n",
            "FROM account\n",
            "INNER JOIN customer ON account.custid = customer.custid;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Medium Level queries\n",
        "question = 'Write a query to display account number, customer’s number, customer’s firstname,lastname,account opening date.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table1} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhsZbZAvKmlK",
        "outputId": "f45461bf-4ccc-49a3-d369-647b46a3d08c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT account.acnumber, customer.fname, customer.ltname\n",
            "FROM account\n",
            "INNER JOIN customer ON account.custid = customer.custid\n",
            "WHERE account.astatus = 'Active';\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = 'Write a query to display account id, customer’s firstname, customer’s lastname for the customer’s whose account is Active.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table1} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z20gj-UIKTUU",
        "outputId": "e94d6ba8-0801-4a05-edca-4ee7c276c532"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT fname, city, acnumber\n",
            "FROM customer\n",
            "INNER JOIN account ON customer.custid = account.custid\n",
            "INNER JOIN branch ON account.bid = branch.bid\n",
            "WHERE gender = 'Female'\n",
            "AND occupation NOT IN ('Business', 'Service', 'Studies')\n",
            "AND account.atype NOT IN ('Business', 'Service', 'Studies');\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#complex queries\n",
        "question = 'Write a query to display the female customers firstname, city and account number who are not into business, service or studies.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table1} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFSYcxZNKc5U",
        "outputId": "dbd379fe-bb7c-4f0f-ac97-f81b44c873d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT c.custid, c.fname, a.acnumber\n",
            "FROM customer c\n",
            "JOIN account a ON c.custid = a.custid\n",
            "JOIN trandetails t ON a.acnumber = t.acnumber\n",
            "WHERE t.dot > DATE_FORMAT(NOW(), '%Y-%m-15')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = 'Write a query to display  the customer number, customer firstname,account number for the customer’s whose accounts were created after 15th of any month.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table1} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZvTLkwMKhwA",
        "outputId": "36adf825-bdfc-46d7-927a-07c5f53e4ac5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "SELECT customer.custid, customer.fname, branch.bid, loan.loan_amount\n",
            "FROM customer\n",
            "JOIN branch ON customer.custid = branch.bid\n",
            "JOIN loan ON customer.custid = loan.custid\n",
            "WHERE loan.loan_amount IS NOT NULL;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = 'Write a query to display the customer’s number, customer’s firstname, branch id and loan amount for people who have taken loans.'\n",
        "\n",
        "prompt = f\"[INST] Write MYSQL query to answer the following question given the database schema. Please wrap your code answer using ```: Schema: {table1} Question: {question} [/INST] Here is the MYSQL query to answer to the question: {question}: ``` \"\n",
        "\n",
        "ans = pipeline(prompt, max_new_tokens=200)\n",
        "print(ans[0]['generated_text'].split('```')[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SelP7IxRozBp"
      },
      "source": [
        "###Natural Language to Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHeeLvXAo2qN",
        "outputId": "e451a224-7eaa-4ef2-f9e1-979eb22bc4a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in python\\n<</SYS>>\\n\\nWrite a function for shadow removal of the image[/INST]  Here's a possible implementation of a function for shadow removal in Python using OpenCV:\n",
            "```\n",
            "import cv2\n",
            "\n",
            "def remove_shadows(image):\n",
            "    # Convert the image to grayscale\n",
            "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
            "    \n",
            "    # Apply a Gaussian blur to the grayscale image\n",
            "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
            "    \n",
            "    # Use the Canny edge detection algorithm to find the edges in the image\n",
            "    edges = cv2.Canny(blurred, 50, 150)\n",
            "    \n",
            "    # Find the contours of the edges\n",
            "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
            "    \n",
            "    # Iterate through the contours and draw a rectangle around each one\n",
            "    for contour in contours:\n",
            "        x, y, w, h = cv2.boundingRect(contour)\n",
            "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
            "    \n",
            "    # Return the image with the shadows removed\n",
            "    return image\n",
            "```\n",
            "This function takes an image as input and returns the image with the shadows removed. It first converts the image to grayscale, applies a Gaussian blur to the grayscale image, and then uses the Canny edge detection algorithm to find the edges in the image. It then finds the contours of the edges and draws a rectangle around each one. Finally, it returns the image with the shadows removed.\n",
            "\n",
            "You can use this function by calling it with the path to an image file as an argument, like this:\n",
            "```\n",
            "image = cv2.imread(\"image.jpg\")\n",
            "shadow_removed_image = remove_shadows(image)\n",
            "cv2.imwrite(\"shadow_removed_image.jpg\", shadow_removed_image)\n",
            "```\n",
            "This will read the image from the file, remove the shadows, and save the resulting image to a new file.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in python\"\n",
        "user = \"Write a function for shadow removal of the image\"\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GsA779-o8M5",
        "outputId": "18ddff1a-203a-4c28-f6ab-95a69828fcd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in python\\n<</SYS>>\\n\\nWrite a function for doing a logistic regression on a binary labelled dataset[/INST]  Here is an example of how you could implement logistic regression in Python using the scikit-learn library:\n",
            "```\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "# Load the iris dataset\n",
            "iris = load_iris()\n",
            "\n",
            "# Split the dataset into training and testing sets\n",
            "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
            "\n",
            "# Create a logistic regression model\n",
            "model = LogisticRegression()\n",
            "\n",
            "# Train the model on the training data\n",
            "model.fit(X_train, y_train)\n",
            "\n",
            "# Make predictions on the testing data\n",
            "y_pred = model.predict(X_test)\n",
            "\n",
            "# Evaluate the model's performance\n",
            "accuracy = model.score(X_test, y_test)\n",
            "print(\"Accuracy:\", accuracy)\n",
            "```\n",
            "This code will train a logistic regression model on the iris dataset and evaluate its performance on the testing data. The `LogisticRegression` class from scikit-learn is used to implement the logistic regression algorithm. The `train_test_split` function from scikit-learn is used to split the dataset into training and testing sets. The `fit` method is used to train the model on the training data, and the `predict` method is used to make predictions on the testing data. The `score` method is used to evaluate the model's performance on the testing data.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in python\"\n",
        "user = \"Write a function for doing a logistic regression on a binary labelled dataset\"\n",
        "\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd3JMjD4o_Td",
        "outputId": "2783748a-6b05-4d30-c4df-186d01e72e31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in python\\n<</SYS>>\\n\\nOCR on image in python using easyocr library[/INST]  To perform OCR (Optical Character Recognition) on an image using the EasyOCR library in Python, you can follow these steps:\n",
            "\n",
            "1. Install the EasyOCR library by running the following command in your terminal or command prompt:\n",
            "```\n",
            "pip install easyocr\n",
            "```\n",
            "2. Import the EasyOCR library in your Python script:\n",
            "```python\n",
            "import easyocr\n",
            "```\n",
            "3. Load the image file that you want to perform OCR on. You can do this using the `cv2.imread()` function from the OpenCV library:\n",
            "```python\n",
            "import cv2\n",
            "\n",
            "image = cv2.imread(\"image.jpg\")\n",
            "```\n",
            "4. Create an instance of the `easyocr.Reader` class and pass the image to the `readtext()` method:\n",
            "```python\n",
            "reader = easyocr.Reader()\n",
            "text = reader.readtext(image)\n",
            "```\n",
            "The `readtext()` method returns a list of strings, where each string represents a line of text in the image.\n",
            "\n",
            "5. You can then print the text to the console or save it to a file:\n",
            "```python\n",
            "print(text)\n",
            "```\n",
            "Here's the complete code:\n",
            "```python\n",
            "import cv2\n",
            "import easyocr\n",
            "\n",
            "# Load the image\n",
            "image = cv2.imread(\"image.jpg\")\n",
            "\n",
            "# Create an instance of the EasyOCR reader\n",
            "reader = easyocr.Reader()\n",
            "\n",
            "# Perform OCR on the image\n",
            "text = reader.readtext(image)\n",
            "\n",
            "# Print the text to the console\n",
            "print(text)\n",
            "```\n",
            "Note that the `readtext()` method can also take additional parameters such as `language` and `scale`, which can be used to specify the language of the text and the scale at which the image should be processed, respectively. For example:\n",
            "```python\n",
            "text = reader.readtext(image, language=\"eng\", scale=2.0)\n",
            "```\n",
            "This will perform OCR on the image using the English language model and a scale factor of 2.0.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in python\"\n",
        "user = \"OCR on image in python using easyocr library\"\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es0CEbJFpEJX",
        "outputId": "c9a32aea-628d-48d0-d7c6-cad595fc3f6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in python\\n<</SYS>>\\n\\ncreate graph using pyneo[/INST]  To create a graph using PyNeo, you can use the `py2neo.create_graph()` function. This function takes a dictionary of graph properties as an argument, and returns a `Graph` object.\n",
            "\n",
            "Here's an example of how you can use `py2neo.create_graph()` to create a graph:\n",
            "```\n",
            "import py2neo\n",
            "\n",
            "# Create a graph\n",
            "graph = py2neo.create_graph({\n",
            "    \"name\": \"My Graph\",\n",
            "    \"description\": \"A graph created using PyNeo\"\n",
            "})\n",
            "\n",
            "# Add nodes and edges to the graph\n",
            "node1 = graph.create_node({\"name\": \"Node 1\"})\n",
            "node2 = graph.create_node({\"name\": \"Node 2\"})\n",
            "edge = graph.create_edge(node1, node2, {\"label\": \"knows\"})\n",
            "\n",
            "# Print the graph\n",
            "print(graph)\n",
            "```\n",
            "This will create a graph with two nodes and one edge, and print the graph to the console.\n",
            "\n",
            "You can also use the `py2neo.create_graph()` function to create a graph from a file. For example:\n",
            "```\n",
            "import py2neo\n",
            "\n",
            "# Create a graph from a file\n",
            "graph = py2neo.create_graph(\"my_graph.json\")\n",
            "\n",
            "# Print the graph\n",
            "print(graph)\n",
            "```\n",
            "This will create a graph from the JSON file `my_graph.json`, and print the graph to the console.\n",
            "\n",
            "You can also use the `py2neo.create_graph()` function to create a graph from a string. For example:\n",
            "```\n",
            "import py2neo\n",
            "\n",
            "# Create a graph from a string\n",
            "graph = py2neo.create_graph(\"\"\"\n",
            "{\n",
            "    \"name\": \"My Graph\",\n",
            "    \"description\": \"A graph created using PyNeo\"\n",
            "}\n",
            "\"\"\")\n",
            "\n",
            "# Print the graph\n",
            "print(graph)\n",
            "```\n",
            "This will create a graph from the JSON string, and print the graph to the console.\n",
            "\n",
            "I hope this helps! Let me know if you have any questions.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in python\"\n",
        "user = \"create graph using pyneo\"\n",
        "\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_yXjIMdpFAT",
        "outputId": "8c01b667-ba38-474d-ddd9-7dc72e17a057"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in python\\n<</SYS>>\\n\\nWrite a function for a binning logic[/INST]  Here is an example of a function that implements a binning logic in Python:\n",
            "```\n",
            "def binning(data, bins):\n",
            "    # Initialize an empty list to store the binned data\n",
            "    binned_data = []\n",
            "\n",
            "    # Loop through the data and bin it according to the specified bins\n",
            "    for i in range(len(data)):\n",
            "        # Get the current value and its corresponding bin index\n",
            "        value = data[i]\n",
            "        bin_index = int(value / bins)\n",
            "\n",
            "        # If the bin index is out of range, set it to the last bin\n",
            "        if bin_index >= len(bins):\n",
            "            bin_index = len(bins) - 1\n",
            "\n",
            "        # Add the value to the corresponding bin\n",
            "        binned_data[bin_index].append(value)\n",
            "\n",
            "    # Return the binned data\n",
            "    return binned_data\n",
            "```\n",
            "This function takes two arguments: `data`, which is a list of values to be binned, and `bins`, which is a list of bin edges. The function loops through the data and bins it according to the specified bins. The bin index is calculated by dividing the value by the number of bins, and the resulting integer is used to index into the `bins` list. If the bin index is out of range, it is set to the last bin. The function then adds the value to the corresponding bin in the `binned_data` list. Finally, the function returns the binned data.\n",
            "\n",
            "Here is an example of how to use this function:\n",
            "```\n",
            "# Define the data to be binned\n",
            "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "\n",
            "# Define the bins\n",
            "bins = [0, 2, 4, 6, 8, 10]\n",
            "\n",
            "# Bin the data\n",
            "binned_data = binning(data, bins)\n",
            "\n",
            "# Print the binned data\n",
            "print(binned_data)\n",
            "```\n",
            "This will output the following:\n",
            "```\n",
            "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]\n",
            "```\n",
            "This shows that the data has been binned into four bins, with the first bin containing the values 1, 2, and 3, the second bin containing the values 4, 5, and 6, and so on.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in python\"\n",
        "user = \"Write a function for a binning logic\"\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIiQHqPepc6t"
      },
      "source": [
        "###Natural Language to Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EpWwh1_pcFF",
        "outputId": "c3c76fec-43b0-4eb1-89f1-634125b58c19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in pyspark\\n<</SYS>>\\n\\nImplementation of Random Forest classifier[/INST]  To implement a Random Forest classifier in PySpark, you can use the `RandomForestClassifier` class from the `ml.spark` module. Here's an example of how to use it:\n",
            "```\n",
            "from pyspark.ml.classification import RandomForestClassifier\n",
            "\n",
            "# Load the data\n",
            "data = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
            "\n",
            "# Split the data into training and test sets\n",
            "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
            "\n",
            "# Create a Random Forest classifier\n",
            "rfc = RandomForestClassifier(numTrees=100, featureSubsetStrategy=\"auto\", impurity='gini', maxDepth=5, maxBins=32)\n",
            "\n",
            "# Train the model on the training data\n",
            "model = rfc.fit(train_data)\n",
            "\n",
            "# Make predictions on the test data\n",
            "predictions = model.transform(test_data)\n",
            "\n",
            "# Evaluate the model\n",
            "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
            "accuracy = evaluator.evaluate(predictions)\n",
            "print(\"Accuracy: \", accuracy)\n",
            "```\n",
            "This code first loads the data from a CSV file using the `read.csv()` method. It then splits the data into training and test sets using the `randomSplit()` method.\n",
            "\n",
            "Next, it creates a `RandomForestClassifier` object with the desired hyperparameters and trains the model on the training data using the `fit()` method.\n",
            "\n",
            "Finally, it makes predictions on the test data using the `transform()` method and evaluates the model's performance using the `MulticlassClassificationEvaluator` class.\n",
            "\n",
            "Note that the `RandomForestClassifier` class has many other hyperparameters that can be tuned for better performance, such as `numTrees`, `featureSubsetStrategy`, `impurity`, `maxDepth`, and `maxBins`. You can experiment with different values for these hyperparameters to see which combination works best for your specific use case.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in pyspark\"\n",
        "user = \"Implementation of Random Forest classifier\"\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64GoU0lXK4t2"
      },
      "source": [
        "###Code Commenting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sC5OaXGK6v2",
        "outputId": "389b28f9-d52c-46a3-8aff-f2c3b947f9b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] please complete the code with comments\n",
            "'''def fibonacci(n):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "def fibonacci_memo(n, memo={}):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1''' [/INST]  Here is the complete code with comments:\n",
            "```\n",
            "def fibonacci(n):\n",
            "    # Base case: If n is 0 or 1, return the value\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        # Recursive case: If n is greater than 1, return the sum of the previous two values\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "def fibonacci_memo(n, memo={}):\n",
            "    # Base case: If n is 0 or 1, return the value\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        # Check if the value is already in the memoization table\n",
            "        if n in memo:\n",
            "            return memo[n]\n",
            "        else:\n",
            "            # If not, calculate the value and add it to the memoization table\n",
            "            memo[n] = fibonacci(n-1) + fibonacci(n-2)\n",
            "            return memo[n]\n",
            "```\n",
            "The `fibonacci` function is the same as the one you provided, but with comments added to explain what each line does.\n",
            "\n",
            "The `fibonacci_memo` function is similar, but it uses a memoization table to store the values of `fibonacci` for previous inputs. This allows the function to avoid recalculating the same values multiple times, which can make the function much faster for large inputs.\n",
            "\n",
            "The `memo` dictionary is initialized to an empty dictionary, and it is used to store the values of `fibonacci` for previous inputs. The `if n in memo` statement checks if the value is already in the memoization table, and if it is, the function returns the value from the table. If the value is not in the table, the function calculates the value and adds it to the table before returning it.\n"
          ]
        }
      ],
      "source": [
        "user = \"\"\"please complete the code with comments\n",
        "'''def fibonacci(n):\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    elif n == 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "def fibonacci_memo(n, memo={}):\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    elif n == 1:\n",
        "        return 1'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] {user.strip()} [/INST]\"\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpJLGMgzM7zO",
        "outputId": "1e9926ea-eae4-4ad9-ede2-8f397626983f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in python\\n<</SYS>>\\n\\nWrite a function for shadow removal of the image with docstring and inline comments[/INST]  \"\"\"\n",
            "\n",
            "  Shadow Removal\n",
            "\n",
            "  This function removes shadows from an image using the following steps:\n",
            "\n",
            "  1. Convert the image to grayscale\n",
            "  2. Apply a Gaussian filter to the grayscale image to reduce noise\n",
            "  3. Apply a thresholding algorithm to the filtered image to separate the shadows from the rest of the image\n",
            "  4. Fill in the shadows with a color of your choice\n",
            "\n",
            "  Args:\n",
            "      image (numpy array): The input image\n",
            "      threshold (int, optional): The threshold value for separating shadows from the rest of the image. Defaults to 127.\n",
            "      fill_color (tuple, optional): The color to fill in the shadows. Defaults to (0, 0, 0).\n",
            "\n",
            "  Returns:\n",
            "      numpy array: The output image with shadows removed\n",
            "\n",
            "  \"\"\"\n",
            "  import numpy as np\n",
            "  from PIL import Image\n",
            "\n",
            "  def remove_shadows(image, threshold=127, fill_color=(0, 0, 0)):\n",
            "      # Convert the image to grayscale\n",
            "      gray = np.array(image.convert('L'))\n",
            "\n",
            "      # Apply a Gaussian filter to the grayscale image to reduce noise\n",
            "      blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
            "\n",
            "      # Apply a thresholding algorithm to the filtered image to separate shadows from the rest of the image\n",
            "      _, thresh = cv2.threshold(blurred, threshold, 255, cv2.THRESH_BINARY)\n",
            "\n",
            "      # Fill in the shadows with a color of your choice\n",
            "      cv2.fillConvexPoly(image, thresh, fill_color)\n",
            "\n",
            "      return image\n",
            "\n",
            "  # Example usage\n",
            "  image = Image.open('image.jpg')\n",
            "  output = remove_shadows(image)\n",
            "  output.save('output.jpg')\n",
            "```\n",
            "This function takes an image as input, converts it to grayscale, applies a Gaussian filter to reduce noise, and then applies a thresholding algorithm to separate shadows from the rest of the image. Finally, it fills in the shadows with a color of your choice. The function returns the output image with shadows removed.\n",
            "\n",
            "You can use this function by calling it with the input image and any desired parameters, such as the threshold value and fill color. For example:\n",
            "```\n",
            "image = Image.open('image.jpg')\n",
            "output = remove_shadows(image, threshold=127, fill_color=(0, 0, 0))\n",
            "output.save('output.jpg')\n",
            "```\n",
            "This will remove shadows from the input image and save the output image to a file called \"output.jpg\". You can adjust the threshold value and fill color to suit your needs.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in python\"\n",
        "user = \"Write a function for shadow removal of the image with docstring and inline comments\"\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSSFBOLsLEOm",
        "outputId": "69c30c04-8f48-497c-b13e-6750cf11eea4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] please complete the code with comments\n",
            "'''def binarySearch(array, x, low, high):\n",
            "    while low <= high:\n",
            "        mid = low + (high - low)//2\n",
            "        if array[mid] == x:\n",
            "            return mid\n",
            "        elif array[mid] < x:\n",
            "            low = mid + 1\n",
            "        else:\n",
            "            high = mid - 1\n",
            "    return -1\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x, 0, len(array)-1)\n",
            "if result != -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")''' [/INST]  Here is the completed code with comments:\n",
            "```\n",
            "def binarySearch(array, x, low, high):\n",
            "    # Check if the element is present in the array\n",
            "    while low <= high:\n",
            "        mid = low + (high - low)//2\n",
            "        if array[mid] == x:\n",
            "            return mid\n",
            "        # If the element is less than the midpoint, search the left half of the array\n",
            "        elif array[mid] < x:\n",
            "            low = mid + 1\n",
            "        # If the element is greater than the midpoint, search the right half of the array\n",
            "        else:\n",
            "            high = mid - 1\n",
            "    # If the element is not found, return -1\n",
            "    return -1\n",
            "\n",
            "# Test the function with an example array and element\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x, 0, len(array)-1)\n",
            "if result!= -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")\n",
            "```\n",
            "This code defines a function `binarySearch` that takes an array, an element `x`, and two indices `low` and `high` as input. The function uses a binary search algorithm to find the index of the element in the array. If the element is found, the function returns the index of the element. If the element is not found, the function returns -1.\n",
            "\n",
            "The function uses a while loop to iterate through the array, starting at the midpoint of the range of indices specified by `low` and `high`. If the element is found at the midpoint, the function returns the index of the element. If the element is less than the midpoint, the function searches the left half of the array by setting `low` to the midpoint plus 1. If the element is greater than the midpoint, the function searches the right half of the array by setting `high` to the midpoint minus 1.\n",
            "\n",
            "The function continues to iterate until the element is found or the range of indices is empty. If the element is not found, the function returns -1.\n",
            "\n",
            "The code then tests the function with an example array and element. If the element is found, the function prints a message indicating the index of the element. If the element is not found, the function prints a message indicating that the element is not present in the array.\n"
          ]
        }
      ],
      "source": [
        "user = \"\"\"please complete the code with comments\n",
        "'''def binarySearch(array, x, low, high):\n",
        "    while low <= high:\n",
        "        mid = low + (high - low)//2\n",
        "        if array[mid] == x:\n",
        "            return mid\n",
        "        elif array[mid] < x:\n",
        "            low = mid + 1\n",
        "        else:\n",
        "            high = mid - 1\n",
        "    return -1\n",
        "array = [3, 4, 5, 6, 7, 8, 9]\n",
        "x = 4\n",
        "result = binarySearch(array, x, 0, len(array)-1)\n",
        "if result != -1:\n",
        "    print(\"Element is present at index \" + str(result))\n",
        "else:\n",
        "    print(\"Not found\")'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] {user.strip()} [/INST]\"\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7NI_DHuLG3V"
      },
      "source": [
        "###Code Discussion and Explainibility\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eVGOQmOLGUM",
        "outputId": "56e1dccc-02d1-4f5e-9ffa-f481c720c458"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] Explain the following code in english\n",
            "'''\n",
            "import cv2\n",
            "import numpy as np\n",
            "image1 = cv2.imread('input1.jpg')\n",
            "img = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
            "ret, thresh1 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY)\n",
            "ret, thresh2 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY_INV)\n",
            "ret, thresh3 = cv2.threshold(img, 120, 255, cv2.THRESH_TRUNC)\n",
            "ret, thresh4 = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO)\n",
            "ret, thresh5 = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO_INV)\n",
            "cv2.imshow('Binary Threshold', thresh1)\n",
            "cv2.imshow('Binary Threshold Inverted', thresh2)\n",
            "cv2.imshow('Truncated Threshold', thresh3)\n",
            "cv2.imshow('Set to 0', thresh4)\n",
            "cv2.imshow('Set to 0 Inverted', thresh5)\n",
            "if cv2.waitKey(0) & 0xff == 27:\n",
            "    cv2.destroyAllWindows()  ''' [/INST]  This code is using the OpenCV library to perform image processing operations on an input image. Here's a breakdown of what each line does:\n",
            "\n",
            "1. `import cv2`: This line imports the OpenCV library, which is a computer vision library that provides functions for image processing and analysis.\n",
            "2. `import numpy as np`: This line imports the NumPy library, which is a library for numerical computing in Python.\n",
            "3. `image1 = cv2.imread('input1.jpg')`: This line reads an image file named \"input1.jpg\" and stores it in the variable `image1`.\n",
            "4. `img = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)`: This line converts the image `image1` from its original color space (BGR) to grayscale. The `cvtColor` function is used to perform the conversion.\n",
            "5. `ret, thresh1 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY)`: This line applies a binary thresholding operation to the grayscale image `img`. The `threshold` function is used to perform the thresholding. The `ret` variable stores the threshold value, and the `thresh1` variable stores the thresholded image.\n",
            "6. `ret, thresh2 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY_INV)`: This line applies a binary thresholding operation to the grayscale image `img` with the opposite threshold value. The `threshold` function is used to perform the thresholding. The `ret` variable stores the threshold value, and the `thresh2` variable stores the thresholded image.\n",
            "7. `ret, thresh3 = cv2.threshold(img, 120, 255, cv2.THRESH_TRUNC)`: This line applies a truncated thresholding operation to the grayscale image `img`. The `threshold` function is used to perform the thresholding. The `ret` variable stores the threshold value, and the `thresh3` variable stores the thresholded image.\n",
            "8. `ret, thresh4 = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO)`: This line applies a thresholding operation to the grayscale image `img` where pixels below the threshold value are set to 0. The `threshold` function is used to perform the thresholding. The `ret` variable stores the threshold value, and the `thresh4` variable stores the thresholded image.\n",
            "9. `ret, thresh5 = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO_INV)`: This line applies a thresholding operation to the grayscale image `img` where pixels above the threshold value are set to 0. The `threshold` function is used to perform the thresholding. The `ret` variable stores the threshold value, and the `thresh5` variable stores the thresholded image.\n",
            "10. `cv2.imshow('Binary Threshold', thresh1)`: This line displays the thresholded image `thresh1` in a window with the title \"Binary Threshold\".\n",
            "11. `cv2.imshow('Binary Threshold Inverted', thresh2)`: This line displays the thresholded image `thresh2` in a window with the title \"Binary Threshold Inverted\".\n",
            "12. `cv2.imshow('Truncated Threshold', thresh3)`: This line displays the thresholded image `thresh3\n"
          ]
        }
      ],
      "source": [
        "user = \"\"\"Explain the following code in english\n",
        "'''\n",
        "import cv2\n",
        "import numpy as np\n",
        "image1 = cv2.imread('input1.jpg')\n",
        "img = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "ret, thresh1 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY)\n",
        "ret, thresh2 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY_INV)\n",
        "ret, thresh3 = cv2.threshold(img, 120, 255, cv2.THRESH_TRUNC)\n",
        "ret, thresh4 = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO)\n",
        "ret, thresh5 = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO_INV)\n",
        "cv2.imshow('Binary Threshold', thresh1)\n",
        "cv2.imshow('Binary Threshold Inverted', thresh2)\n",
        "cv2.imshow('Truncated Threshold', thresh3)\n",
        "cv2.imshow('Set to 0', thresh4)\n",
        "cv2.imshow('Set to 0 Inverted', thresh5)\n",
        "if cv2.waitKey(0) & 0xff == 27:\n",
        "    cv2.destroyAllWindows()  '''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] {user.strip()} [/INST]\"\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1200,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn1xbIMirIlZ",
        "outputId": "5d8a9d32-b375-4113-e52b-b4481384fc85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] Explain the following code in english\n",
            "'''\n",
            "from numpy import loadtxt\n",
            "from xgboost import XGBClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
            "# split data into X and y\n",
            "X = dataset[:,0:8]\n",
            "Y = dataset[:,8]\n",
            "seed = 7\n",
            "test_size = 0.33\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
            "model = XGBClassifier()\n",
            "model.fit(X_train, y_train)\n",
            "y_pred = model.predict(X_test)\n",
            "predictions = [round(value) for value in y_pred]\n",
            "accuracy = accuracy_score(y_test, predictions)\n",
            "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))''' [/INST]  This code is using the XGBoost library to train a machine learning model on a dataset of diabetes patient data. The dataset is loaded from a CSV file using the `loadtxt` function from NumPy.\n",
            "\n",
            "The code then splits the data into training and testing sets using the `train_test_split` function from Scikit-learn. The training set is used to train the model, and the testing set is used to evaluate its performance.\n",
            "\n",
            "The model is then trained using the `fit` method of the `XGBClassifier` class. The `predict` method is then used to make predictions on the testing set.\n",
            "\n",
            "Finally, the accuracy of the model is calculated using the `accuracy_score` function from Scikit-learn, and the results are printed to the console.\n",
            "\n",
            "Here's a breakdown of the code:\n",
            "\n",
            "1. `from numpy import loadtxt`: This line imports the `loadtxt` function from NumPy, which is used to load the dataset from a CSV file.\n",
            "2. `from xgboost import XGBClassifier`: This line imports the `XGBClassifier` class from the XGBoost library, which is used to train the machine learning model.\n",
            "3. `from sklearn.model_selection import train_test_split`: This line imports the `train_test_split` function from Scikit-learn, which is used to split the data into training and testing sets.\n",
            "4. `from sklearn.metrics import accuracy_score`: This line imports the `accuracy_score` function from Scikit-learn, which is used to calculate the accuracy of the model.\n",
            "5. `dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")`: This line loads the dataset from a CSV file using the `loadtxt` function from NumPy. The `delimiter` parameter is set to `,` to indicate that the dataset is comma-separated.\n",
            "6. `X = dataset[:,0:8]`: This line extracts the first 8 columns of the dataset, which are used as the input features for the model.\n",
            "7. `Y = dataset[:,8]`: This line extracts the 9th column of the dataset, which is used as the output feature (i.e., the target variable).\n",
            "8. `seed = 7`: This line sets the random seed for the `train_test_split` function to 7. This ensures that the same split is used every time the code is run.\n",
            "9. `test_size = 0.33`: This line sets the size of the testing set to 33%.\n",
            "10. `X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)`: This line splits the data into training and testing sets using the `train_test_split` function from Scikit-learn. The `test_size` parameter is set to 0.33, which means that 33% of the data will be used for testing and 67% will be used for training. The `random_state` parameter is set to 7, which ensures that the same split is used every time the code is run.\n",
            "11. `model = XGBClassifier()`: This line creates an instance of the `XGBClass\n"
          ]
        }
      ],
      "source": [
        "user = \"\"\"Explain the following code in english\n",
        "'''\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
        "# split data into X and y\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "seed = 7\n",
        "test_size = 0.33\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "predictions = [round(value) for value in y_pred]\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] {user.strip()} [/INST]\"\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_O4t-mzNQLa"
      },
      "source": [
        "###Code Conversion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEyHN2nRNxoO",
        "outputId": "53215f52-3969-4448-e17b-3a3396ff7078"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in pyspark\\n<</SYS>>\\n\\nconvert the following python code to pyspark\n",
            "'''\n",
            "def binarySearch(array, x, low, high):\n",
            "    while low <= high:\n",
            "        mid = low + (high - low)//2\n",
            "        if array[mid] == x:\n",
            "            return mid\n",
            "        elif array[mid] < x:\n",
            "            low = mid + 1\n",
            "        else:\n",
            "            high = mid - 1\n",
            "    return -1\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x, 0, len(array)-1)\n",
            "if result != -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")\n",
            "'''\n",
            "[/INST]  Here's the equivalent PySpark code:\n",
            "```python\n",
            "from pyspark.sql.functions import col\n",
            "\n",
            "def binarySearch(array, x, low, high):\n",
            "    while low <= high:\n",
            "        mid = low + (high - low)//2\n",
            "        if array[mid] == x:\n",
            "            return mid\n",
            "        elif array[mid] < x:\n",
            "            low = mid + 1\n",
            "        else:\n",
            "            high = mid - 1\n",
            "    return -1\n",
            "\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x, 0, len(array)-1)\n",
            "if result!= -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")\n",
            "```\n",
            "Note that in PySpark, you can use the `col` function to create a column from a list, and the `range` function to create a range of numbers.\n",
            "\n",
            "Also, in PySpark, you can use the `filter` function to filter a DataFrame based on a condition, and the `select` function to select a subset of columns from a DataFrame.\n",
            "\n",
            "So, the equivalent PySpark code for the given Python code would be:\n",
            "```python\n",
            "from pyspark.sql.functions import col, range, filter, select\n",
            "\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x, 0, len(array)-1)\n",
            "if result!= -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")\n",
            "```\n",
            "Note that in PySpark, you can also use the `binarySearch` function to perform a binary search on a DataFrame.\n",
            "\n",
            "So, the equivalent PySpark code for the given Python code would be:\n",
            "```python\n",
            "from pyspark.sql.functions import col, range, filter, select, binarySearch\n",
            "\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x, 0, len(array)-1)\n",
            "if result!= -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in pyspark\"\n",
        "user = \"\"\"convert the following python code to pyspark\n",
        "'''\n",
        "def binarySearch(array, x, low, high):\n",
        "    while low <= high:\n",
        "        mid = low + (high - low)//2\n",
        "        if array[mid] == x:\n",
        "            return mid\n",
        "        elif array[mid] < x:\n",
        "            low = mid + 1\n",
        "        else:\n",
        "            high = mid - 1\n",
        "    return -1\n",
        "array = [3, 4, 5, 6, 7, 8, 9]\n",
        "x = 4\n",
        "result = binarySearch(array, x, 0, len(array)-1)\n",
        "if result != -1:\n",
        "    print(\"Element is present at index \" + str(result))\n",
        "else:\n",
        "    print(\"Not found\")\n",
        "'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"Provide answers in Java\"\n",
        "user = \"\"\"convert the following python code to Java\n",
        "'''\n",
        "def binarySearch(array, x, low, high):\n",
        "    while low <= high:\n",
        "        mid = low + (high - low)//2\n",
        "        if array[mid] == x:\n",
        "            return mid\n",
        "        elif array[mid] < x:\n",
        "            low = mid + 1\n",
        "        else:\n",
        "            high = mid - 1\n",
        "    return -1\n",
        "array = [3, 4, 5, 6, 7, 8, 9]\n",
        "x = 4\n",
        "result = binarySearch(array, x, 0, len(array)-1)\n",
        "if result != -1:\n",
        "    print(\"Element is present at index \" + str(result))\n",
        "else:\n",
        "    print(\"Not found\")\n",
        "'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY_iYywnfbnz",
        "outputId": "5a4595cd-7a89-45c1-d5db-10c1eb5f4bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in Java\\n<</SYS>>\\n\\nconvert the following python code to Java\n",
            "'''\n",
            "def binarySearch(array, x, low, high):\n",
            "    while low <= high:\n",
            "        mid = low + (high - low)//2\n",
            "        if array[mid] == x:\n",
            "            return mid\n",
            "        elif array[mid] < x:\n",
            "            low = mid + 1\n",
            "        else:\n",
            "            high = mid - 1\n",
            "    return -1\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x, 0, len(array)-1)\n",
            "if result != -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")\n",
            "'''\n",
            "[/INST]  Here is the Java equivalent of the given Python code:\n",
            "```\n",
            "import java.util.Arrays;\n",
            "\n",
            "public class BinarySearch {\n",
            "    public static int binarySearch(int[] array, int x, int low, int high) {\n",
            "        while (low <= high) {\n",
            "            int mid = low + (high - low) / 2;\n",
            "            if (array[mid] == x) {\n",
            "                return mid;\n",
            "            } else if (array[mid] < x) {\n",
            "                low = mid + 1;\n",
            "            } else {\n",
            "                high = mid - 1;\n",
            "            }\n",
            "        }\n",
            "        return -1;\n",
            "    }\n",
            "\n",
            "    public static void main(String[] args) {\n",
            "        int[] array = {3, 4, 5, 6, 7, 8, 9};\n",
            "        int x = 4;\n",
            "        int result = binarySearch(array, x, 0, array.length - 1);\n",
            "        if (result!= -1) {\n",
            "            System.out.println(\"Element is present at index \" + result);\n",
            "        } else {\n",
            "            System.out.println(\"Not found\");\n",
            "        }\n",
            "    }\n",
            "}\n",
            "```\n",
            "Note that in Java, the `Arrays.binarySearch()` method can be used instead of the custom implementation, which would simplify the code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ArZHL7rg2B"
      },
      "source": [
        "###Code Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypWE2Lx-rkC-",
        "outputId": "790f32e4-3ac0-41cd-d95d-89157a44a7e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in pyspark\\n<</SYS>>\\n\\nPlease optimize the following code to reduce runtime\n",
            "'''\n",
            "def binarySearch(array, x, low, high):\n",
            "    while low <= high:\n",
            "        mid = low + (high - low)//2\n",
            "        if array[mid] == x:\n",
            "            return mid\n",
            "        elif array[mid] < x:\n",
            "            low = mid + 1\n",
            "        else:\n",
            "            high = mid - 1\n",
            "    return -1\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x, 0, len(array)-1)\n",
            "if result != -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")\n",
            "'''\n",
            "\n",
            "[/INST]  The code can be optimized by using the `bisect` module in PySpark, which provides a more efficient implementation of the binary search algorithm. Here's the optimized code:\n",
            "```python\n",
            "from bisect import bisect_left\n",
            "\n",
            "def binarySearch(array, x):\n",
            "    i = bisect_left(array, x)\n",
            "    if i!= len(array) and array[i] == x:\n",
            "        return i\n",
            "    else:\n",
            "        return -1\n",
            "\n",
            "array = [3, 4, 5, 6, 7, 8, 9]\n",
            "x = 4\n",
            "result = binarySearch(array, x)\n",
            "if result!= -1:\n",
            "    print(\"Element is present at index \" + str(result))\n",
            "else:\n",
            "    print(\"Not found\")\n",
            "```\n",
            "The `bisect_left` function from the `bisect` module is used to perform the binary search. It returns the index of the first element in the array that is greater than or equal to the target value `x`. If the target value is not found in the array, the function returns the index of the first element that is greater than the target value.\n",
            "\n",
            "The optimized code is more efficient because it avoids the overhead of repeatedly calling the `binarySearch` function with the same arguments. Instead, it uses the `bisect_left` function to perform the binary search, which is optimized for large arrays.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in python\"\n",
        "user = \"\"\"Please optimize the following code to reduce runtime\n",
        "'''\n",
        "def binarySearch(array, x, low, high):\n",
        "    while low <= high:\n",
        "        mid = low + (high - low)//2\n",
        "        if array[mid] == x:\n",
        "            return mid\n",
        "        elif array[mid] < x:\n",
        "            low = mid + 1\n",
        "        else:\n",
        "            high = mid - 1\n",
        "    return -1\n",
        "array = [3, 4, 5, 6, 7, 8, 9]\n",
        "x = 4\n",
        "result = binarySearch(array, x, 0, len(array)-1)\n",
        "if result != -1:\n",
        "    print(\"Element is present at index \" + str(result))\n",
        "else:\n",
        "    print(\"Not found\")\n",
        "'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh88UVvtsPLr",
        "outputId": "f8d676b4-689e-49de-90c4-17c08792d412"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in python\\n<</SYS>>\\n\\nPlease optimize the following code to reduce runtime\n",
            "'''\n",
            "input_list = [1, 2, -3]\n",
            "output_list = []\n",
            "for x in input_list:\n",
            "  if x >= 0:\n",
            "    output_list.append(1)\n",
            "  else:\n",
            "    output_list.append(0)\n",
            "'''\n",
            "[/INST]  Here's a more efficient way to write the same code in Python:\n",
            "```\n",
            "output_list = [1 if x >= 0 else 0 for x in input_list]\n",
            "```\n",
            "This uses a list comprehension to create a new list with the same length as `input_list`, where each element is either 1 or 0 depending on whether the corresponding element in `input_list` is greater than or equal to 0.\n",
            "\n",
            "This approach is more efficient because it avoids the need to iterate over the input list multiple times, and it also avoids the need to create and append elements to the output list. Instead, it creates the entire output list in one go, which can be much faster for large input lists.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in python\"\n",
        "user = \"\"\"Please optimize the following code to reduce runtime\n",
        "'''\n",
        "input_list = [1, 2, -3]\n",
        "output_list = []\n",
        "for x in input_list:\n",
        "  if x >= 0:\n",
        "    output_list.append(1)\n",
        "  else:\n",
        "    output_list.append(0)\n",
        "'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsdL0OcUCcjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1491b563-8ba8-4b68-ab86-57118e1d2d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in MySQl\\n<</SYS>>\\n\\nPlease optimize the following query to reduce runtime\n",
            "'''\n",
            "SELECT customer_id,count(customer_id)\n",
            "FROM sales\n",
            "GROUP BY customer_id\n",
            "HAVING customer_id != '16' AND customer_id != '2';\n",
            "'''\n",
            "[/INST]  The query can be optimized by using a subquery to filter the customer IDs instead of using the `HAVING` clause. Here's the optimized query:\n",
            "```\n",
            "SELECT customer_id, count(customer_id)\n",
            "FROM sales\n",
            "WHERE customer_id NOT IN (16, 2)\n",
            "GROUP BY customer_id;\n",
            "```\n",
            "This query uses a subquery to filter the customer IDs to exclude `16` and `2`, and then groups the results by customer ID. This should reduce the runtime of the query.\n"
          ]
        }
      ],
      "source": [
        "system = \"Provide answers in MySQl\"\n",
        "user = \"\"\"Please optimize the following query to reduce runtime\n",
        "'''\n",
        "SELECT customer_id,count(customer_id)\n",
        "FROM sales\n",
        "GROUP BY customer_id\n",
        "HAVING customer_id != '16' AND customer_id != '2';\n",
        "'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"Provide answers in MySQl\"\n",
        "user = \"\"\"Please optimize the following query to reduce runtime\n",
        "'''\n",
        "SELECT *\n",
        "FROM products p\n",
        "WHERE p.product_id =\n",
        "(SELECT s.product_id\n",
        "FROM sales s\n",
        "WHERE s.customer_id = 2468\n",
        "AND s.quantity_sold = 12 );\n",
        "'''\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfuq7qq8eyjI",
        "outputId": "cbf00c2c-fd7e-4163-ba5e-b59da7c1879c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nProvide answers in MySQl\\n<</SYS>>\\n\\nPlease optimize the following query to reduce runtime\n",
            "'''\n",
            "SELECT *\n",
            "FROM products p\n",
            "WHERE p.product_id =\n",
            "(SELECT s.product_id\n",
            "FROM sales s\n",
            "WHERE s.customer_id = 2468\n",
            "AND s.quantity_sold = 12 );\n",
            "'''\n",
            "[/INST]  The query can be optimized by using a subquery with a join instead of a subquery with a correlated subquery. The optimized query would be:\n",
            "```\n",
            "SELECT p.*\n",
            "FROM products p\n",
            "JOIN sales s ON p.product_id = s.product_id\n",
            "WHERE s.customer_id = 2468 AND s.quantity_sold = 12;\n",
            "```\n",
            "This query uses a join to retrieve the product information for the products that have been sold by customer 2468 with a quantity of 12. The join is more efficient than the correlated subquery because it only requires scanning the sales table once, whereas the correlated subquery requires scanning the sales table for each product in the products table.\n",
            "\n",
            "Additionally, the optimized query uses the `JOIN` keyword instead of the `WHERE` clause to specify the join condition, which makes the query more readable and easier to understand.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Code Fixing"
      ],
      "metadata": {
        "id": "oZbCg11Xg-uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"Please fix the code\"\n",
        "user = \"\"\"I am developing a Django site and have been having trouble trying to work out the best way to do exception handling. I have been doing\n",
        "'''\n",
        "try:\n",
        "    Some code\n",
        "except:\n",
        "    log error in my own words, i.e 'Some code' failed to execute\n",
        "    Some other code'''\n",
        "This catches all exceptions thus ensuring my site does not deliver 500 errors and such like.\n",
        "But, with my limited knowledge I am losing the actual exception and it is making it a real pain to debug.\n",
        "How do I print the error that occured? Currently I comment out try: catch: and see the error and fix it\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx5-VIcCgwSj",
        "outputId": "9df473b2-6004-4625-f17c-0e76eb2dc07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nPlease fix the code\\n<</SYS>>\\n\\nI am developing a Django site and have been having trouble trying to work out the best way to do exception handling. I have been doing\n",
            "'''\n",
            "try:\n",
            "    Some code\n",
            "except:\n",
            "    log error in my own words, i.e 'Some code' failed to execute\n",
            "    Some other code'''\n",
            "This catches all exceptions thus ensuring my site does not deliver 500 errors and such like. \n",
            "But, with my limited knowledge I am losing the actual exception and it is making it a real pain to debug. \n",
            "How do I print the error that occured? Currently I comment out try: catch: and see the error and fix it\n",
            "[/INST]  To print the actual exception that occurred, you can use the `except` block to catch the exception and then print it using the `print()` function. Here's an example:\n",
            "```\n",
            "try:\n",
            "    Some code\n",
            "except Exception as e:\n",
            "    print(e)\n",
            "    log error in my own words, i.e 'Some code' failed to execute\n",
            "    Some other code\n",
            "```\n",
            "This will print the actual exception that occurred, along with its message, to the console.\n",
            "\n",
            "Alternatively, you can use the `logging` module to log the exception and its message. Here's an example:\n",
            "```\n",
            "import logging\n",
            "\n",
            "try:\n",
            "    Some code\n",
            "except Exception as e:\n",
            "    logging.error(e)\n",
            "    log error in my own words, i.e 'Some code' failed to execute\n",
            "    Some other code\n",
            "```\n",
            "This will log the exception and its message to the `error.log` file in your project's root directory.\n",
            "\n",
            "You can also use the `traceback` module to get a more detailed information about the exception, including the line number and file where the exception occurred. Here's an example:\n",
            "```\n",
            "import traceback\n",
            "\n",
            "try:\n",
            "    Some code\n",
            "except Exception as e:\n",
            "    print(traceback.format_exc())\n",
            "    log error in my own words, i.e 'Some code' failed to execute\n",
            "    Some other code\n",
            "```\n",
            "This will print the full traceback of the exception, including the line number and file where the exception occurred, to the console.\n",
            "\n",
            "It's important to note that you should only catch the specific exceptions that you know how to handle, and let other exceptions propagate up the call stack. Catching all exceptions with a single `except` block can mask the actual problem and make it harder to debug.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"Please fix the code\"\n",
        "user = \"\"\"I faced an error when I run my program using python: The error is like this:\n",
        "ZeroDivisionError: division by zero\n",
        "My codes looks like below\n",
        "'''\n",
        "x = 0\n",
        "y = 0\n",
        "z = x/y'''\n",
        "how to avoid that error in python. My desired output is z = 0 and see the error and fix it\n",
        "\"\"\"\n",
        "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
        "#inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOnbdyIKg86x",
        "outputId": "38db7fd3-a020-4d4b-de2a-17b8bc8c2d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: <s>[INST] <<SYS>>\\nPlease fix the code\\n<</SYS>>\\n\\nI faced an error when I run my program using python: The error is like this:\n",
            "ZeroDivisionError: division by zero\n",
            "My codes looks like below\n",
            "'''\n",
            "x = 0\n",
            "y = 0\n",
            "z = x/y'''\n",
            "how to avoid that error in python. My desired output is z = 0 and see the error and fix it\n",
            "[/INST]  The error you are seeing is because you are trying to divide by zero, which is not allowed in Python. To fix this error, you can add a check to make sure that `y` is not equal to zero before dividing. Here is an example of how you can modify your code to avoid this error:\n",
            "```\n",
            "x = 0\n",
            "y = 0\n",
            "z = 0\n",
            "if y!= 0:\n",
            "    z = x/y\n",
            "else:\n",
            "    print(\"Error: division by zero\")\n",
            "```\n",
            "This code will first check if `y` is equal to zero. If it is, it will print an error message and set `z` to zero. If `y` is not equal to zero, it will divide `x` by `y` and set `z` to the result.\n",
            "\n",
            "Alternatively, you can use the built-in `try` and `except` blocks to catch the error and handle it appropriately. Here is an example of how you can modify your code to use this approach:\n",
            "```\n",
            "x = 0\n",
            "y = 0\n",
            "z = 0\n",
            "try:\n",
            "    z = x/y\n",
            "except ZeroDivisionError:\n",
            "    print(\"Error: division by zero\")\n",
            "```\n",
            "This code will try to divide `x` by `y` and set `z` to the result. If an error occurs, it will catch the error and print an error message.\n",
            "\n",
            "I hope this helps! Let me know if you have any questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CM3iw1Q3iFy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f0559695339845eaba4053ad7e30bcd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_240ecf9a717b4241921aed601bb2afda",
              "IPY_MODEL_cdc05ffa260c4d72bdf40be4f3b5e184",
              "IPY_MODEL_ef821d69aed341d286d9eb4c24562163"
            ],
            "layout": "IPY_MODEL_6afb62a83b8c416188fd4a35c952266e"
          }
        },
        "240ecf9a717b4241921aed601bb2afda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4ba95c06549430e8b11fcd01bc5b82a",
            "placeholder": "​",
            "style": "IPY_MODEL_9b9d56eb63e042fb9c80de1230b0289f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cdc05ffa260c4d72bdf40be4f3b5e184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a0261337e864660a73268bb943de221",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b66f57389ab7480eb67445178ed57b93",
            "value": 2
          }
        },
        "ef821d69aed341d286d9eb4c24562163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faf4c58570f74f28baf634c3019f42af",
            "placeholder": "​",
            "style": "IPY_MODEL_248ce63d6cdc423f9f7ccfb5b288e1d6",
            "value": " 2/2 [01:13&lt;00:00, 33.71s/it]"
          }
        },
        "6afb62a83b8c416188fd4a35c952266e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ba95c06549430e8b11fcd01bc5b82a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9d56eb63e042fb9c80de1230b0289f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a0261337e864660a73268bb943de221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b66f57389ab7480eb67445178ed57b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faf4c58570f74f28baf634c3019f42af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "248ce63d6cdc423f9f7ccfb5b288e1d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}